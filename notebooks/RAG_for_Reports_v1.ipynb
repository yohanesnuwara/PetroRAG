{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yohanesnuwara/PetroRAG/blob/main/notebooks/RAG_for_Reports_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PetroRAG v1\n",
        "\n",
        "\n",
        "\n",
        "PetroRAG is Retrieval Augmented Generation or RAG supports for Oil and Gas reports question & answering. It can load multiple files. This the first generation, using OpenAI service.\n",
        "\n",
        "Credit: [Yohanes Nuwara](https://github.com/yohanesnuwara/PetroRAG)\n",
        "\n"
      ],
      "metadata": {
        "id": "KvTS2_RkLkWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup (click Play to run)\n",
        "\n",
        "!pip -q install openai==0.28\n",
        "!pip -q install pypdf2\n",
        "!pip -q install tiktoken\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "import openai\n",
        "import PyPDF2\n",
        "import nltk\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "Jv_nKcvhj9Dr",
        "outputId": "8d706509-ce26-49cf-de44-d4140452ad23"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Input your OpenAI API Key\n",
        "# @markdown Find here: https://www.howtogeek.com/885918/how-to-get-an-openai-api-key/\n",
        "\n",
        "API_key = ''  # @param {type: \"string\"}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lDc2S14gktSQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Process documents\n",
        "\n",
        "# OpenAI API key\n",
        "openai.api_key = API_key\n",
        "\n",
        "# Model setup\n",
        "COMPLETIONS_MODEL = \"gpt-3.5-turbo\"\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "\n",
        "# Encoding generator\n",
        "MAX_SECTION_LEN = 500\n",
        "SEPARATOR = \"\\n* \"\n",
        "ENCODING = \"cl100k_base\"  # Suitable for gpt-3.5-turbo and gpt-4\n",
        "\n",
        "\n",
        "# Functions\n",
        "def trim(text, n_start, n_end):\n",
        "  # Split the sentence into a list of sentences\n",
        "  sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "  # Slice the list to only include the first two sentences\n",
        "  trimmed_sentences = sentences[n_start:n_end]\n",
        "\n",
        "  # Join the sliced list of sentences back into a string\n",
        "  trimmed_sentence = \" \".join(trimmed_sentences)\n",
        "\n",
        "  return trimmed_sentence\n",
        "\n",
        "def get_embedding(text: str, model: str=EMBEDDING_MODEL) -> list[float]:\n",
        "    result = openai.Embedding.create(\n",
        "      model=model,\n",
        "      input=text\n",
        "    )\n",
        "    return result[\"data\"][0][\"embedding\"]\n",
        "\n",
        "def compute_doc_embeddings(df: pd.DataFrame) -> dict[tuple[str, str], list[float]]:\n",
        "    \"\"\"\n",
        "    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.\n",
        "\n",
        "    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n",
        "    \"\"\"\n",
        "    embeddings = {}\n",
        "    for idx, r in tqdm(df.iterrows(), total=len(df), desc=\"Computing embeddings\"):\n",
        "        embeddings[idx] = get_embedding(r.Text)\n",
        "    return embeddings\n",
        "\n",
        "def vector_similarity(x: list[float], y: list[float]) -> float:\n",
        "    \"\"\"\n",
        "    Returns the similarity between two vectors.\n",
        "\n",
        "    Because OpenAI Embeddings are normalized to length 1, the cosine similarity is the same as the dot product.\n",
        "    \"\"\"\n",
        "    return np.dot(np.array(x), np.array(y))\n",
        "\n",
        "def order_document_sections_by_query_similarity(query: str, contexts: dict[(str, str), np.array]) -> list[(float, (str, str))]:\n",
        "    \"\"\"\n",
        "    Find the query embedding for the supplied query, and compare it against all of the pre-calculated document embeddings\n",
        "    to find the most relevant sections.\n",
        "\n",
        "    Return the list of document sections, sorted by relevance in descending order.\n",
        "    \"\"\"\n",
        "    query_embedding = get_embedding(query)\n",
        "\n",
        "    document_similarities = sorted([\n",
        "        (vector_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in contexts.items()\n",
        "    ], reverse=True)\n",
        "\n",
        "    return document_similarities\n",
        "\n",
        "def construct_prompt(question: str, context_embeddings: dict, df: pd.DataFrame) -> str:\n",
        "    \"\"\"\n",
        "    Fetch relevant\n",
        "    \"\"\"\n",
        "    most_relevant_document_sections = order_document_sections_by_query_similarity(question, context_embeddings)\n",
        "\n",
        "    chosen_sections = []\n",
        "    chosen_sections_len = 0\n",
        "    chosen_sections_indexes = []\n",
        "\n",
        "    for _, section_index in most_relevant_document_sections:\n",
        "        # Add contexts until we run out of space.\n",
        "        document_section = df.loc[section_index]\n",
        "\n",
        "        chosen_sections_len += 100 + separator_len\n",
        "        if chosen_sections_len > MAX_SECTION_LEN:\n",
        "            break\n",
        "\n",
        "        chosen_sections.append(SEPARATOR + document_section.Text.replace(\"\\n\", \" \"))\n",
        "        chosen_sections_indexes.append(str(section_index))\n",
        "\n",
        "    # Useful diagnostic information\n",
        "    # print(f\"Selected {len(chosen_sections)} document sections:\")\n",
        "    # print(\"\\n\".join(chosen_sections_indexes))\n",
        "\n",
        "    header = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\"\n",
        "\n",
        "    return header + \"\".join(chosen_sections) + \"\\n\\n Q: \" + question + \"\\n A:\"\n",
        "\n",
        "\n",
        "\n",
        "def answer_query_with_context(\n",
        "    query: str,\n",
        "    df: pd.DataFrame,\n",
        "    document_embeddings: dict[tuple[str, str], np.ndarray],\n",
        "    show_prompt: bool = False\n",
        ") -> str:\n",
        "    # Construct the prompt using your custom logic\n",
        "    prompt = construct_prompt(\n",
        "        query,\n",
        "        document_embeddings,\n",
        "        df\n",
        "    )\n",
        "\n",
        "    if show_prompt:\n",
        "        print(\"Constructed Prompt:\\n\", prompt)\n",
        "\n",
        "    # Use the ChatCompletion API with messages\n",
        "    response = openai.ChatCompletion.create(\n",
        "        # model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        **COMPLETIONS_API_PARAMS\n",
        "    )\n",
        "\n",
        "    # Extract and return the response content\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"].strip() + \"mlkaxklaxqpxoqkpoqa\"\n",
        "\n",
        "\"\"\"\n",
        "Read files\n",
        "\"\"\"\n",
        "\n",
        "# Specify the directory containing the PDF files\n",
        "pdf_directory = \"/content\"\n",
        "\n",
        "# Use glob to find all PDF files in the specified directory\n",
        "pdf_paths = glob.glob(os.path.join(pdf_directory, \"*.PDF\")) + glob.glob(os.path.join(pdf_directory, \"*.pdf\"))\n",
        "\n",
        "# Create the pdf_files list\n",
        "pdf_files = []\n",
        "for idx, file_path in enumerate(pdf_paths, start=1):\n",
        "    # Assign an iterative name like file1, file2, file3, ...\n",
        "    file_name = f\"file{idx}\"\n",
        "    pdf_files.append([file_name, file_path])\n",
        "\n",
        "\"\"\"\n",
        "Text chunking\n",
        "\"\"\"\n",
        "\n",
        "# Create an empty list to store DataFrames\n",
        "dfs = []\n",
        "\n",
        "# Set interval\n",
        "interval = 10\n",
        "\n",
        "# Iterate over pdf_files with tqdm for progress tracking\n",
        "for f in tqdm(pdf_files, desc=\"Creating chunks of texts from reports\"):\n",
        "    company_name = f[0]\n",
        "    pdf_file = open(f[1], \"rb\")\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "    text = \"\"\n",
        "\n",
        "    # Extract text from PDF pages\n",
        "    for page in range(len(pdf_reader.pages)):\n",
        "        text += pdf_reader.pages[page].extract_text()\n",
        "\n",
        "    # Clean and format document\n",
        "    training_data = text.replace(\"\\n\", \" \")\n",
        "\n",
        "    # Tokenize sentences using NLTK\n",
        "    sentences = nltk.sent_tokenize(training_data)\n",
        "\n",
        "    # Trim sentences and append to dfs\n",
        "    for l in range(0, len(sentences), interval):\n",
        "        trimmed_sentence = trim(training_data, l, l + interval)\n",
        "\n",
        "        # Append a new DataFrame to the list with the company name and trimmed sentence\n",
        "        dfs.append(pd.DataFrame({\"Article_ID\": [f\"{company_name}_{l}\"], \"Text\": [trimmed_sentence]}))\n",
        "\n",
        "# Concatenate all DataFrames into a single DataFrame\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "print(\"\\n\")\n",
        "\n",
        "\"\"\"\n",
        "Compute embeddings\n",
        "\"\"\"\n",
        "\n",
        "embed_df = compute_doc_embeddings(df)\n",
        "\n",
        "\"\"\"\n",
        "Answer query\n",
        "\"\"\"\n",
        "\n",
        "encoding = tiktoken.get_encoding(ENCODING)\n",
        "separator_len = len(encoding.encode(SEPARATOR))\n",
        "\n",
        "COMPLETIONS_API_PARAMS = {\n",
        "    # Temperature of 0.5 for balanced outputs.\n",
        "    \"temperature\": 0.9,\n",
        "    \"max_tokens\": 2000,\n",
        "    \"model\": \"gpt-3.5-turbo\",  # Replace with your desired model\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "MdDOYuudoKWD",
        "outputId": "ab0b8cd3-84c8-4aac-8f84-1041fd7ebe9a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating chunks of texts from reports: 100%|██████████| 2/2 [00:13<00:00,  6.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing embeddings: 100%|██████████| 161/161 [00:34<00:00,  4.61it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ask the report\n",
        "# @markdown Put your question or prompt here:\n",
        "\n",
        "Question = 'can you describe the drilling activities?'  # @param {type: \"string\"}\n",
        "\n",
        "# Process the prompt\n",
        "answer_query_with_context(Question, df, embed_df, show_prompt=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "cellView": "form",
        "id": "foapk3RtsESQ",
        "outputId": "8bc69687-268c-417c-d019-34de921e7a42"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The drilling activities involved drilling an 8½\" hole from 3210m to 3473m, taking pressure points and circulating the hole clean, POOH with 8½ BHA to shoe at 2531m, laying out the same BHA, and drilling to total depth (TD) at 3498m. Additionally, activities included laying out an 8 1/2\" drilling BHA and running the casing with various specifications and procedures such as cleaning, drifting, measuring, and doping at Statoil onshore base.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}